{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatbot with Deep Learning\n",
    "\n",
    "## Ask Chatbot to get answer based on previous reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.models import Sequential,load_model\n",
    "from tensorflow.keras.layers import Dense,LSTM,GRU,Embedding,Dropout,Flatten\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pickle import dump,load\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONSTANTS\n",
    "#EMBEDDING_DIM = 100\n",
    "#VALIDATION_SPLIT = 0.3\n",
    "SAMPLE_SIZE = 0.5\n",
    "#MAX_SEQUENCE_LENGTH = 100\n",
    "FILE_PATH = '../../6 - NPL files/yelp_training_set_review.json'\n",
    "#GLOVE_DIR = '../../../Machine Learning documents/glove/'\n",
    "\n",
    "#variables\n",
    "train_text_len = 25 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_file = pd.read_csv(FILE_PATH)\n",
    "data_file = pd.read_json(FILE_PATH, lines=True)\n",
    "data_file = data_file[data_file['stars'] == 1].sample(frac=SAMPLE_SIZE).copy()\n",
    "all_text = data_file['text'].values\n",
    "all_text = all_text.astype('str')\n",
    "num_instances = all_text.shape[0]\n",
    "all_text = '. '.join(all_text.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en',disable=['parser', 'tagger','ner'])\n",
    "nlp.max_length = num_instances * 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_punc(doc_text):\n",
    "    return [token.text.lower() for token in nlp(doc_text) if token.text not in '\\n\\n \\n\\n\\n!\"-#$%&()--.*+,-/:;<=>?@[\\\\]^_`{|}~\\t\\n ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = separate_punc(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_sequences = []\n",
    "for i in range(train_text_len,len(tokens)):\n",
    "    seq = tokens[i-train_text_len:i]\n",
    "    text_sequences.append(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(text_sequences)\n",
    "sequences = tokenizer.texts_to_sequences(text_sequences)\n",
    "vocabulary_size = len(tokenizer.word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 : i\n",
      "964 : purchased\n",
      "1504 : carpet\n",
      "11 : for\n",
      "321 : three\n",
      "13076 : bedrooms\n",
      "58 : from\n",
      "14 : this\n",
      "235 : home\n",
      "2422 : depot\n",
      "188 : location\n",
      "13 : we\n",
      "696 : moved\n",
      "49 : all\n",
      "8 : of\n",
      "1 : the\n",
      "1908 : furniture\n",
      "44 : as\n",
      "119 : well\n",
      "44 : as\n",
      "1 : the\n",
      "2898 : built\n",
      "10 : in\n",
      "30301 : shelving\n",
      "10 : in\n",
      "48 : one\n"
     ]
    }
   ],
   "source": [
    "for i in sequences[0]:\n",
    "    print(f'{i} : {tokenizer.index_word[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = np.array(sequences) \n",
    "X = sequences[:,:-1]\n",
    "y = sequences[:,-1]\n",
    "y = to_categorical(y, num_classes=vocabulary_size+1)\n",
    "seq_len = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(vocabulary_size, seq_len):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocabulary_size, 25, input_length=seq_len))\n",
    "    model.add(LSTM(150, return_sequences=True))\n",
    "    model.add(LSTM(150))\n",
    "    model.add(Dense(150, activation='relu'))\n",
    "\n",
    "    model.add(Dense(vocabulary_size, activation='softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "   \n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 25, 25)            757550    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 25, 150)           105600    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 150)               180600    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 150)               22650     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30302)             4575602   \n",
      "=================================================================\n",
      "Total params: 5,642,002\n",
      "Trainable params: 5,642,002\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model(vocabulary_size+1, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, y, batch_size=128, epochs=300,verbose=1)\n",
    "model.save('gen_text.h5')\n",
    "dump(tokenizer,open('gen_text_tokenizer,â€™wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
